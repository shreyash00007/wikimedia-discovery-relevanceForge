"""Parse elasticsearch explains into tensorflow graphs

Lucene explains contain the full scoring equation that was used at runtime,
including similarity parameters and plenty of metrics that were looked up to
answer the query.  Tuning a scoring function by re-running the queries is
possible but very slow. With the values extracted from the explains and a
tensorflow graph of the equation built up simple scoring functions can be run
at speeds in excess of 1M hits/sec on a modest laptop.

The quick evaluation of millions of hits opens up new possibilities in tuning
the scoring queries.

IMPORTANT: This is very alpha stage, and likely only works for the query
parameters  and explains explicitly spelled out in the test suite. Some
elasticsearch queries, like `query_string`, will likely never be supported
here.

== Overview

1) Parse the source elasticsearch query into a parser specialized
   to that query
2) Parse explains for individual hits into explain objects. Feature
   vectors can be extracted from these.
3) Merge explains from many hits until all parsers created in step 1
   are satisfied they have seen the full scoring equation.
4) Convert the merged explain into a tensorflow graph

== elasticsearch query -> explain parser

The primary entry point here is RootExplainParser.from_query. This accepts the
root of an elasticserach query, including `query` and `rescore` keys, and
generates a parser.  Individual query parsers are registered with the
`register_parser` decorator. Generic elasticsearch queries  are parsed using
the registered parsers by calling `explain_parser_from_query`

== explain parser -> explain

This is a complete mess, and is buried in assumptions about what the explains looked like
when writing the code. The assertions scattered everywhere hopefully cover the assumptions
made and will catch changes in updated lucene versions that break these assumptions.

Each ExplainParser has a `parse` method that accepts a single explain detail. It must
either accept it and return an Explain object, or raise an IncorrectExplainException.
The IncorrectExplainException's are necessary because there is no direct link between
the explain and the original nodes in the graph. For example a bool query could have
4 should statements but only 1 matches. When run through the `parse_list` function
each parser will be tried against the explain until one accepts it.

For the most part the parsers check the description and maybe a little structure
to decide if they should throw an IncorrectExplainException. Once that decision
is made assertions are used to verify things look the way they are expected.

== feature vector extraction

Most of the feature vector handling is via `BaseExplain.feature_vec`.
This merges the feature vectors of child explains, applies naming prefixes, and
asserts no duplicate feature vectors are being returned. The most common
way to add a value to the feature vector is for the parser to generate a
PassThruExplain object, which will turn the `value` field of the explain
into a feature vector element.

Feature vectors are always returned as a list of 1 or more floats, never
a scalar. This is necessary to support explains with varied length feature
vectors, specifically match queries which can match many individual terms.
At feature extraction times the width of these vectors can vary, and they
are padded with 0's when running the graph.

== explain merging

Explain merging is the process of taking two explains generated by the same ExplainParser
and asking the parser to merge them. When generating explains the original parser
is attached as a property so we have access to the right parser to merge with.

Most child merging is handled with the `merge_children` function which will
look through two lists of children, separate into the unique set of parsers,
and merge parsers with multiple elements.

Explain merging is the reason we need ExplainParsers specialized to each query
type, the explains from multiple hits that trigger different parts of a query
are hard to reason about directly. We need the extra context of knowing
approximately what lucene queries were constructed under the hood and what they
should look like to merge explains reliably.

== convert to tensorflow graph

Converting to a tensorflow graph is straight forward once the above is all worked
out. For the most part explains will either take a product or sum of their children,
and specialized explains (tfNorm, dismax, etc) implement custom logic that
recreates the appropriate scoring equation.

"""
from collections import defaultdict
from functools import reduce

import tensorflow as tf

from relforge.explain_parser.utils import (
    isclose, join_name, name_fixer, clean_newlines)


# Full explain parser implementations
PARSERS = {}


def register_parser(name):
    def inner(fn):
        assert name not in PARSERS
        PARSERS[name] = fn
        return fn
    return inner


def explain_parser_from_query(query):
    """Create an explain parser from an elasticsearch query"""
    assert len(query) == 1
    query_type = next(iter(query.keys()))
    if query_type not in PARSERS:
        raise Exception('Unsupported query: {}'.format(query_type))
    options = query[query_type]
    return PARSERS[query_type](options)


@register_parser("template")
def parser_for_template_query(options):
    # Templates are invisible in the returned explain
    # TODO: apply template?
    return explain_parser_from_query(options['inline'])


class IncorrectExplainException(Exception):
    pass


def num_complete_children(children):
    return sum(1 for child in children if child.is_complete)


def parse_list(available_parsers, lucene_details, catch_errors=True):
    """Apply a list of explain parsers to a list of details"""
    remaining_parsers = list(available_parsers)
    parsed = []
    remaining_details = []
    for child in lucene_details:
        for i, parser in enumerate(remaining_parsers):
            try:
                parsed_child = parser.parse(child)
            except IncorrectExplainException:
                # We dont really know what parsers apply to what details, so
                # the standard case is to catch errors and try another parser.
                # This can be disabled for debugging to help figure out why
                # something wont parse.
                if not catch_errors:
                    raise
                continue
            remaining_parsers.pop(i)
            # None is returned for non-contributing explains
            if parsed_child is not None:
                parsed.append(parsed_child)
            break
        else:
            remaining_details.append(child)
    return remaining_parsers, remaining_details, parsed


def merge_children(a, b, all_parsers):
    # Match up children with our parsers
    parsers_by_hash = {hash(p): p for p in all_parsers}
    grouped = defaultdict(list)
    for name, children in (('a', a), ('b', b)):
        for child in children:
            if child.parser_hash not in parsers_by_hash:
                raise Exception('Unexpected parser not in all_parsers: {}'.format(child.parser))
            grouped[child.parser_hash].append((name, child))
    # Merge together anything we can
    all_children = []
    for parser_hash, children in grouped.items():
        if len(children) == 1:
            all_children.append(children[0][1])
        elif len(children) == 2:
            # sanity check these didn't come from the same side of the merge.
            child_names = {name for name, child in children}
            assert 2 == len(child_names)
            parser = parsers_by_hash[parser_hash]
            # TODO: explain why these indices
            all_children.append(parser.merge(children[0][1], children[1][1]))
        else:
            raise Exception('Expected 1 or 2 children per parser')
    return all_children


class BaseExplainParser(object):
    def parse(self, lucene_explain):
        """Parse a lucene explain node.

        Recieves a lucene explanation node and either returns
        a BaseExplain or raises an IncorrectExplainException
        when the explain is not parsable.
        """
        raise NotImplementedError(type(self))

    def merge(self, a, b):
        """Merge two explains returned by self.parse.

        ????
        """
        raise NotImplementedError(type(self))


class RootExplainParser(BaseExplainParser):
    """Represents the entirety of an elasticsearch query.

    RootExplainParser is the entry point of the explain parsing system.  The
    from_query method accepts the same json as the elasticsearch _search
    endpoint and the parse method accepts the _explanation field of a hit
    generated by running _search with that query.
    """
    def __init__(self, root, rescore, query_weight, rescore_query_weight):
        self.root = root
        self.rescore = rescore
        self.query_weight = query_weight
        self.rescore_query_weight = rescore_query_weight

    @classmethod
    def from_query(cls, root):
        query = explain_parser_from_query(root['query'])
        rescore = None
        query_weight = None
        rescore_query_weight = None
        if 'rescore' in root:
            assert len(root['rescore']) == 1, "TODO: support more rescores"
            assert root['rescore'][0].get('score_mode', 'total') == 'total', "TODO: support more score_mode's"
            query_weight = root['rescore'][0]['query'].get('query_weight', 1)
            rescore_query_weight = root['rescore'][0]['query'].get('rescore_query_weight', 1)
            rescore = explain_parser_from_query(root['rescore'][0]['query']['rescore_query'])
        return cls(query, rescore, query_weight, rescore_query_weight)

    def _parse_query(self, lucene_explain):
        # There might be a type filter, or maybe not. It depends on the url used.
        if len(lucene_explain['details']) > 1:
            maybe_type_filter = lucene_explain['details'][1]
            if self.check_type_filter(maybe_type_filter):
                assert lucene_explain['description'] == 'sum of:'
                assert len(lucene_explain['details']) == 2
                lucene_explain = lucene_explain['details'][0]
        return self.root.parse(lucene_explain)

    def _parse_rescore(self, lucene_explain):
        # TODO: Is this ever not present?
        if lucene_explain['description'] == 'product of:' and len(lucene_explain['details']) == 2 \
                and lucene_explain['details'][1]['description'] == 'secondaryWeight':
            assert isclose(lucene_explain['details'][1]['value'], self.rescore_query_weight)
            lucene_explain = lucene_explain['details'][0]
        return self.rescore.parse(lucene_explain)

    def parse(self, lucene_explain):
        lucene_explain = clean_newlines(lucene_explain)
        if not self.rescore:
            return self._parse_query(lucene_explain)

        assert lucene_explain['description'] == 'sum of:', "TODO: support other rescore methods"
        assert len(lucene_explain['details']) == 2
        query_lucene_explain, rescore_lucene_explain = lucene_explain['details']
        rescore_query = self._parse_rescore(rescore_lucene_explain)
        if query_lucene_explain['description'] == 'product of:' and len(query_lucene_explain['details']) == 2 \
                and query_lucene_explain['details'][1]['description'] == 'primaryWeight':
            assert isclose(query_lucene_explain['details'][1]['value'], self.query_weight)
            query_lucene_explain = query_lucene_explain['details'][0]
        else:
            raise Exception('Didnt find expected rescore query weight')

        weighted_rescore_children = [
            rescore_query,
            TunableVariableExplain(
                {
                    'value': self.rescore_query_weight,
                    'description': 'rescore_query_weight',
                },
                name='rescore_query_weight'
            )]
        rescore_query_weighted = ProductExplain(
            {
                'value': rescore_query.value * self.rescore_query_weight,
                'description': 'product of:',
            },
            name='rescore',
            expected_children=len(weighted_rescore_children),
            children=weighted_rescore_children)

        query = self._parse_query(query_lucene_explain)
        weighted_query_children = [
            query,
            TunableVariableExplain(
                {
                    'value': self.query_weight,
                    'description': 'query_weight',
                },
                name='query_weight'
            )]
        query_weighted = ProductExplain(
            {
                'value': query.value * self.query_weight,
                'description': 'product of:',
            },
            name='query',
            expected_children=len(weighted_query_children),
            children=weighted_query_children)

        explain = SumExplain(lucene_explain, children=[query_weighted, rescore_query_weighted], expected_children=2)
        explain.parser_hash = hash(self)
        return explain

    def merge(self, a, b):
        """Merge two explains returned by self.parse"""
        if not self.rescore:
            return self.root.merge(a, b)

        # Quick double check these are explains we expect
        for explain in (a, b):
            assert isinstance(explain, SumExplain)
            for child in explain.children:
                assert isinstance(child, ProductExplain)
                assert len(child.children) == 2
                assert isinstance(child.children[1], TunableVariableExplain)

        a_query, a_rescore = a.children
        b_query, b_rescore = b.children

        a_query.children[0] = self.root.merge(a_query.children[0], b_query.children[0])
        a_rescore.children[0] = self.rescore.merge(a_rescore.children[0], b_rescore.children[0])
        return a

    def check_type_filter(self, lucene_explain):
        return (
            lucene_explain['description'] == 'match on required clause, product of:'
            and lucene_explain['value'] == 0
            and len(lucene_explain['details']) == 2
            and lucene_explain['details'][0]['description'] == '# clause'
            and lucene_explain['details'][1]['description'] in ('#*:* -_type:__*, product of:', '_type:page, product of:'))  # noqa: E501


class BaseExplain(object):
    """BaseExplain and it's subclasses represent the parsed explain hierarchy.

    TODO: WRITE MORE!!!
    """
    def __init__(self, lucene_explain, children=[], expected_children=None, name=None):
        self.description = lucene_explain['description']
        self.value = float(lucene_explain['value'])
        self.children = children
        if expected_children is not None:
            self.expected_children = expected_children
        elif len(children) == 0:
            self.expected_children = 0
        else:
            raise Exception('expected_children must be provided')
        if name is None:
            self.name = None
        else:
            self.name = name_fixer(name)

    @property
    def children(self):
        return self._children

    @property
    def is_complete(self):
        assert self.expected_children is not None
        return self.expected_children == num_complete_children(self.children)

    @children.setter
    def children(self, value):
        assert value is not None, type(self).__name__
        assert all(isinstance(c, BaseExplain) for c in value), [type(c).__name__ for c in value]
        assert not any(c == self for c in value), "no recursion!"
        self._children = value

    @staticmethod
    def _concat_w_scalars(tensors):
        by_shape = {tuple(t.shape.as_list()): t for t in tensors}
        if len(by_shape) == 2 and () in by_shape:
            # reshape scalar to match type
            by_shape.pop(())
            shape = next(iter(by_shape.values())).shape
            tensors = [t if t.shape == shape else tf.reshape(t, shape) for t in tensors]
        return tf.concat(tensors, axis=1)

    def child_tensor(self, vecs, prefix):
        """Join children into a single tensor

        Parameters
        ----------
        vecs : dict
            dict from name to tensor for that feature vector
        prefix : str
            The prefix to apply to all child tensors. It is assumed self.name
            has already been joined to this.

        Returns
        -------
        tf.Tensor
        """
        child_tensors = [child.to_tf(vecs, prefix) for child in self.children if child]
        child_tensors = [tensor for tensor in child_tensors if tensor is not None]
        if child_tensors:
            return self._concat_w_scalars(child_tensors)
        else:
            return tf.constant(0.0, name=prefix)

    def to_tf(self, vecs, prefix):
        """Transform explain into a tensorflow operation

        Parameters
        ----------
        vecs : dict
            dict from name to tensor for that feature vector
        prefix : str
            String to prefix to all named child nodes. Implementations
            will attach self.name to this prefix.

        Returns
        -------
        tf.Tensor
        """
        raise NotImplementedError(type(self))

    def feature_vec(self, prefix):
        """Extract feature vector from explain

        Parameters
        ----------
        prefix : str
            String to prefix to all named child nodes. Implementations
            will attach self.name to this prefix.

        Returns
        -------
        dict
            dict from name to list of floats for that feature vector
        """
        prefix = join_name(prefix, self.name)
        data = {}
        for child in self.children:
            for k, v in child.feature_vec(prefix).items():
                if k in data:
                    raise Exception('Variable name conflict: {}'.format(k))
                data[k] = v
        return data

    def to_str_arr(self, indent):
        """Recursively convert self into an array of string representations"""
        output = [indent + str(self)]
        if self.children:
            assert all(isinstance(c, BaseExplain) for c in self.children)
            child_indent = indent + '  '
            for child in self.children:
                output.extend(child.to_str_arr(child_indent))
        return output

    def to_str(self, indent=''):
        return '\n'.join(self.to_str_arr(''))

    def __repr__(self):
        return self.to_str()

    def __str__(self):
        return '{}: {:.2f}, {}'.format(type(self).__name__, self.value, self.description)


class TunableVariableExplain(BaseExplain):
    """Treat the explain as a tunable value"""
    is_complete = True

    def __init__(self, *args, **kwargs):
        super(TunableVariableExplain, self).__init__(*args, **kwargs)
        if self.name is None:
            self.name = 'boost'

    def to_tf(self, vecs, prefix):
        return tf.get_variable(join_name(prefix, self.name), initializer=self.value)

    def feature_vec(self, prefix):
        return {}


class PassThruExplain(BaseExplain):
    """Insert the value of an explain into the feature vector"""
    is_complete = True

    def __init__(self, *args, **kwargs):
        super(PassThruExplain, self).__init__(*args, **kwargs)
        if self.name is None:
            self.name = self.description

    def to_tf(self, vecs, prefix):
        return vecs[join_name(prefix, self.name)]

    def feature_vec(self, prefix):
        return {join_name(prefix, self.name): [self.value]}


class GlobalConstantExplain(BaseExplain):
    """Treat the explain as a global constant, such as avgFieldLength"""
    is_complete = True

    def feature_vec(self, prefix):
        return {}

    def to_tf(self, vecs, prefix):
        # TODO: Does name need prefix attached?
        if self.name:
            name = self.name
        else:
            name = self.description.replace(' ', '-')
        return tf.constant(self.value, name=name)


class SumExplain(BaseExplain):
    boost = False  # TODO: What is this?

    def to_tf(self, vecs, prefix):
        prefix = join_name(prefix, self.name)
        # If we have no children this will return a constant scalar
        child_tensor = self.child_tensor(vecs, prefix)
        if child_tensor.shape == ():
            tensor = child_tensor
        else:
            tensor = tf.reduce_sum(child_tensor, 1)
            tensor = tf.reshape(tensor, shape=[-1, 1])
        return tensor


class ProductExplain(BaseExplain):
    def to_tf(self, vecs, prefix):
        prefix = join_name(prefix, self.name)
        if len(self.children) == 1:
            return self.children[0].to_tf(vecs, prefix)

        child_tensors = [child.to_tf(vecs, prefix) for child in self.children if child]
        n_dims = set(len(t.shape) for t in child_tensors)
        n_dims.discard(0)  # A scalar multiplies just fine
        if len(n_dims) != 1:
            for c, t in zip(self.children, child_tensors):
                print(str(c), str(t))
            raise Exception('All child tensors must have same number of dimensions')
        n_dims = n_dims.pop()
        if n_dims > 1:
            # TODO: Why was it tf.reduce_prod doesn't work here?
            tensor = reduce(lambda a, x: tf.multiply(a, x), child_tensors)
        elif any(len(t.shape) > 0 and t.shape[-1] != 1 for t in child_tensors):
            raise Exception('I dont like that tensor shape (why?)')
        else:
            # All children are scalar or single dimensional with a final width of 1
            child_tensor = tf.concat(child_tensors, axis=1)
            tensor = tf.reduce_prod(child_tensor, axis=1)
            tensor = tf.reshape(tensor, [-1, 1])
        return tensor
