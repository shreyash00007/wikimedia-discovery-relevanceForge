{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# For whatever reason setting spark.pyspark.python isn't \n",
    "# being picked up, so we must set in environment\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark2')\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F, types as T\n",
    "\n",
    "spark = SparkSession.builder.master('yarn').getOrCreate()\n",
    "\n",
    "# silly hack makes chaining some transformations cleaner\n",
    "DataFrame.transform = lambda self, fn: fn(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_groups_as_list(name, group_cols, *extra_aggs):\n",
    "    \"\"\"Collect df into a row per group\"\"\"\n",
    "    def transform(df):\n",
    "        # sort for determinism, probably doesn't matter\n",
    "        item_cols = sorted(set(df.columns) - set(group_cols))\n",
    "        return (\n",
    "            df\n",
    "            .groupBy(*group_cols)\n",
    "            .agg(F.collect_list(F.struct(item_cols)).alias(name))\n",
    "        )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def explode_and_flatten_names(col_name):\n",
    "    \"\"\"Inverse of collect_groups_as_list\"\"\"\n",
    "    def transform(df):\n",
    "        assert 'exploded' not in df.columns\n",
    "        dataType = df.schema[col_name].dataType.elementType\n",
    "        base_cols = list(set(df.columns) - {col_name})\n",
    "        element_cols = ['exploded.' + name for name in dataType.fieldNames()]\n",
    "        \n",
    "        return (\n",
    "            df\n",
    "            .select(F.explode(col_name).alias('exploded'), *base_cols)\n",
    "            .select(*(base_cols + element_cols))\n",
    "        )\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_duplicates(events):\n",
    "    seen = set()\n",
    "    for event in events:\n",
    "        if event['uniqueId'] in seen:\n",
    "            continue\n",
    "        seen.add(event['uniqueId'])\n",
    "        yield event\n",
    "\n",
    "        \n",
    "def as_dym_events(events):\n",
    "    \"\"\"Aggregate session into dym events\n",
    " \n",
    "    Takes the full set of events that occured within a single search\n",
    "    session and transforms them into an event per search with various\n",
    "    boolean properties related to dym.\n",
    "    \n",
    "    This could plausibly be done directly in spark, but it seemed\n",
    "    much more complex and required multiple shuffles to aggregate\n",
    "    per-search and per-session data that is needed.\n",
    "    \"\"\"\n",
    "    events = list(events)\n",
    "    # Map from suggested query to search token that suggested it\n",
    "    suggested = {}\n",
    "    for event in events:\n",
    "        if not event['suggestion']:\n",
    "            continue\n",
    "        sugg = event['suggestion']\n",
    "        # First suggestion wins. not amazing but maybe good enough.\n",
    "        if sugg not in suggested or suggested[sugg][0] > event['dt']:\n",
    "            suggested[sugg] = (event['dt'], event['searchToken'])\n",
    "    suggested = {k: v[1] for k, v in suggested.items()}\n",
    "    \n",
    "    # set of searches that showed the result set of a dym query\n",
    "    dym_searches = set()\n",
    "    # set of searches that were autorewritten\n",
    "    dym_autorewrite = set()\n",
    "    # set of searches that showed a dym suggestion\n",
    "    dym_shown = set()\n",
    "    # set of searches that clicked the dym\n",
    "    dym_clicked = set()\n",
    "    # set of searches that had user interaction (click, etc)\n",
    "    hit_interact = set()\n",
    "    # set of searches that displayed 1 or more results\n",
    "    has_results = set()\n",
    "    \n",
    "    for event in events:\n",
    "        is_serp = event['action'] == 'searchResultPage'\n",
    "        is_autorewrite = is_serp and event['didYouMeanVisible'] == 'autorewrite'\n",
    "        is_dym_visible = is_serp and event['didYouMeanVisible'] in ('autorewrite', 'yes')\n",
    "        is_dym_clickthrough = is_serp and event['inputLocation'] in (\n",
    "            'dym-suggest', 'dym-rewritten')\n",
    "        is_dym_cancel_autorewrite = is_serp and event['inputLocation'] == 'dym-original'\n",
    "        is_dym = is_autorewrite or is_dym_clickthrough\n",
    "        is_hit_interact = event['action'] in ('click', 'visitPage')\n",
    "        \n",
    "        if is_dym_visible:\n",
    "            dym_shown.add(event['searchToken'])\n",
    "        if is_dym:\n",
    "            dym_searches.add(event['searchToken'])\n",
    "        if is_autorewrite:\n",
    "            dym_autorewrite.add(event['searchToken'])\n",
    "        # TODO: dym_cancel_autorewrite is clicking the dym area, but it's\n",
    "        # to cancel and get the original query results. Should it count?\n",
    "        if is_dym_clickthrough or is_dym_cancel_autorewrite:\n",
    "            try:\n",
    "                dym_clicked.add(suggested[event['query']])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if is_serp and event['hitsReturned'] is not None and event['hitsReturned'] > 0:\n",
    "            has_results.add(event['searchToken'])\n",
    "        if is_hit_interact:\n",
    "            hit_interact.add(event['searchToken'])\n",
    "        \n",
    "    dym_events = []\n",
    "    # Lazy dedup of tokens + random associated dt.\n",
    "    # TODO: should probably group and take min dt or at least\n",
    "    # something consistent.\n",
    "    search_dt = {event['searchToken']: event['dt'] for event in events}\n",
    "    for search, dt in search_dt.items():\n",
    "        dym_events.append((\n",
    "            dt,\n",
    "            search in dym_autorewrite,\n",
    "            search in dym_searches,\n",
    "            search in dym_shown,\n",
    "            search in dym_clicked,\n",
    "            search in has_results,\n",
    "            search in hit_interact\n",
    "        ))\n",
    "    return dym_events\n",
    "        \n",
    "        \n",
    "@F.udf(returnType=T.ArrayType(T.StructType([\n",
    "    T.StructField('dt', T.StringType(), False),\n",
    "    T.StructField('is_autorewrite', T.BooleanType(), False),\n",
    "    T.StructField('is_dym', T.BooleanType(), False),\n",
    "    T.StructField('dym_shown', T.BooleanType(), False),\n",
    "    T.StructField('dym_clicked', T.BooleanType(), False),\n",
    "    T.StructField('has_resuults', T.BooleanType(), False),\n",
    "    T.StructField('hit_interact', T.BooleanType(), False)\n",
    "])))\n",
    "def transform_session(events):\n",
    "    # Convert a series of source events into very specific events\n",
    "    # about DYM interactions\n",
    "    return as_dym_events(clean_duplicates(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION_COND = (\n",
    "    (F.col('year') == 2019) &\n",
    "    (F.col('month') == 7) &\n",
    "    (F.col('day') >= 7) &\n",
    "    (F.col('day') < 14))\n",
    "\n",
    "# When a user clicks a DYM suggestion the resulting SERP event records\n",
    "# that this is a suggested query, but not which search suggested it. Pull\n",
    "# in the suggested queries so we can at least guess.\n",
    "df_cirrus_sugg = (\n",
    "    spark.read.table('event.mediawiki_cirrussearch_request')\n",
    "    .where(PARTITION_COND)\n",
    "    # Guess what the provided suggestion was.\n",
    "    # TODO: Cirrus should log final suggestion at top level\n",
    "    .select(F.col('search_id').alias('searchToken'),\n",
    "            F.explode(F.col('elasticsearch_requests.suggestion')).alias('suggestion'))\n",
    "    .where(F.col('suggestion').isNotNull())\n",
    "    .groupBy('searchToken').agg(F.first('suggestion').alias('suggestion'))\n",
    ")\n",
    "\n",
    "df_dym = (\n",
    "    spark.read.table('event.testsearchsatisfaction2')\n",
    "    .where(PARTITION_COND)\n",
    "    .where(F.col('event.source') == \"fulltext\")\n",
    "    # checkin events don't contain any useful data for dym.\n",
    "    .where(F.col('event.action') != 'checkin')\n",
    "    .select(\n",
    "        'wiki',\n",
    "        'dt',\n",
    "        'event.action',\n",
    "        'event.didYouMeanVisible',\n",
    "        'event.hitsReturned',\n",
    "        'event.inputLocation',\n",
    "        'event.query',\n",
    "        'event.searchSessionId',\n",
    "        'event.searchToken',\n",
    "        'event.uniqueId')\n",
    "    .join(df_cirrus_sugg, how='left', on='searchToken')\n",
    "    .transform(collect_groups_as_list('events', ['wiki', 'searchSessionId']))\n",
    "    .cache()\n",
    "    .select('wiki', 'searchSessionId',\n",
    "            transform_session(F.col('events')).alias('dym_events'))\n",
    "    .transform(explode_and_flatten_names('dym_events'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dym_local = df_dym.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "from collections import defaultdict\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "\n",
    "def ridge(bucket, data, scale):\n",
    "    return list(zip([bucket]*len(data), scale*data))\n",
    "\n",
    "def plot_dist(title, colors, data, x_range):\n",
    "    min_x = min(np.min(raw) for _, _, raw in data.values())\n",
    "    max_x = max(np.max(raw) for _, _, raw in data.values())\n",
    "    \n",
    "    x = np.linspace(min_x, max_x, 500)\n",
    "    # A bit evil .. but for the patch to draw the polygon we need\n",
    "    # the data must start and end with y=0. The first and last\n",
    "    # x values are repeated and these are applied manually later.\n",
    "    x = np.append(np.append(x, x[-1])[::-1], x[0])[::-1]\n",
    "    source = bokeh.models.ColumnDataSource(data=dict(x=x))\n",
    "    p = bokeh.plotting.figure(\n",
    "        y_range=sorted(data.keys(), reverse=True), title=title,\n",
    "        plot_height=75 * len(data), plot_width=700,\n",
    "        x_range=x_range,\n",
    "        toolbar_location=None)\n",
    "    \n",
    "    pdfs = {bucket: gaussian_kde(raw) for bucket, (_, _, raw) in data.items()}\n",
    "    ys = {bucket: pdf(x) for bucket, pdf in pdfs.items()}\n",
    "    max_y = max(np.max(ys[bucket]) for bucket in data.keys())\n",
    "    scale = 0.8 / max_y\n",
    "    \n",
    "    bounds_data = defaultdict(list)\n",
    "    label_data = defaultdict(list)\n",
    "    for bucket, (bounds, n, raw) in sorted(data.items(), key=lambda x: x[0], reverse=True):\n",
    "        # Apply polygon minimum edges\n",
    "        ys[bucket][0] = 0\n",
    "        ys[bucket][-1] = 0\n",
    "        y = ridge(bucket, ys[bucket], scale=scale)\n",
    "        source.add(y, bucket)\n",
    "        p.patch(\n",
    "            'x', bucket, color=colors[bucket], line_color=\"black\",\n",
    "            alpha=0.6, source=source)\n",
    "        \n",
    "        label_data['bucket'].append(bucket)\n",
    "        label_data['label'].append('n={:,}'.format(n))\n",
    "        \n",
    "        if bounds:\n",
    "            bounds_data['bucket'].append(bucket)\n",
    "            bounds_data['upper'].append(bounds[-1])\n",
    "            bounds_data['lower'].append(bounds[0])\n",
    "            \n",
    "    source_label = bokeh.models.ColumnDataSource(label_data)\n",
    "    p.add_layout(bokeh.models.LabelSet(\n",
    "        x=0, y='bucket', text='label', text_font_size=\"8pt\",\n",
    "        x_offset=-50, y_offset=-23,\n",
    "        render_mode='css', source=source_label))\n",
    "    \n",
    "    if bounds_data:\n",
    "        source_error = bokeh.models.ColumnDataSource(bounds_data)\n",
    "        p.add_layout(bokeh.models.Whisker(\n",
    "            dimension=\"width\", line_color=\"black\",\n",
    "            source=source_error, base=\"bucket\", upper=\"upper\", lower=\"lower\"))\n",
    "\n",
    "    p.y_range.range_padding = 1.0\n",
    "    return p\n",
    "\n",
    "def _ci(values, rounds=1000, alpha=0.05, n=None, agg=lambda x: x.mean(axis=1)):\n",
    "    if n is None:\n",
    "        n = len(values)\n",
    "    samples = np.random.choice(values, size=n * rounds, replace=True).reshape(rounds, -1)\n",
    "    scores = np.sort(agg(samples))\n",
    "    low = int(rounds * (alpha/2))\n",
    "    mid = int(rounds / 2)\n",
    "    high = int(rounds * (1 - alpha/2))\n",
    "    return (scores[low], scores[mid], scores[high]), n, scores\n",
    "\n",
    "def ci(df, extract, **kwargs):\n",
    "    data = {}\n",
    "    buckets = df['bucket'].unique()\n",
    "    for bucket in sorted(buckets):\n",
    "        samples = extract(df[df['bucket'] == bucket])\n",
    "        data[bucket] = _ci(samples, **kwargs)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = df_dym_local[df_dym_local['wiki'] == 'enwiki'].copy()\n",
    "df['bucket'] = 'control'\n",
    "df['non_autorewrite_is_dym'] = df['is_dym'] & ~df['is_autorewrite']\n",
    "df['non_autorewrite_dym_shown'] = df['dym_shown'] & ~df['is_autorewrite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "StatDef = namedtuple('StatDef', ('name', 'filter_col', 'stat_col'))\n",
    "stat_defs = [\n",
    "    StatDef('shown a dym suggestion', None, 'dym_shown'),\n",
    "    StatDef('shown an autorewrite dym', None, 'is_autorewrite'),\n",
    "    StatDef('shown a non-autorewrite dym suggestion', None, 'non_autorewrite_dym_shown'),\n",
    "    StatDef('shown the results of a dym suggestion', None, 'is_dym'),\n",
    "    StatDef('shown the results of a non-autorewrite dym suggestion',\n",
    "            None, 'non_autorewrite_is_dym'),\n",
    "    StatDef('shown non-autorewrite dym suggestion that clicked through to dym results',\n",
    "            'non_autorewrite_dym_shown', 'dym_clicked'),\n",
    "    StatDef('shown autorewrite dym results that clicked a result',\n",
    "            'is_autorewrite', 'hit_interact'),\n",
    "    StatDef('shown non-autorewrite dym results that clicked a result',\n",
    "            'non_autorewrite_is_dym', 'hit_interact'),\n",
    "    StatDef('clicking some result', None, 'hit_interact'),\n",
    "    StatDef('shown a non-autorewrite dym suggestion and clicked non-dym result',\n",
    "            'non_autorewrite_dym_shown', 'hit_interact')\n",
    "]\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "stats = OrderedDict()\n",
    "\n",
    "for stat_def in stat_defs:\n",
    "    df_source = df    \n",
    "    if stat_def.filter_col is not None:\n",
    "        df_source = df_source[df_source[stat_def.filter_col]]\n",
    "    name = '% of search {}'.format(stat_def.name)\n",
    "    stats[name] = ci(df_source, lambda x: x[stat_def.stat_col])\n",
    "    \n",
    "    df_session = df_source.groupby(['bucket', 'searchSessionId'])[stat_def.stat_col].agg(np.any).reset_index()\n",
    "    name = '% of session {}'.format(stat_def.name)\n",
    "    stats[name] = ci(df_session, lambda x: x[stat_def.stat_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bokeh.layouts import column\n",
    "\n",
    "colors = {'control': 'blue'}\n",
    "figs = []\n",
    "for k, v in stats.items():\n",
    "    figs.append(plot_dist(k, colors, v, figs[0].x_range if figs else (0, 0.61)))\n",
    "bokeh.io.show(column(*figs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TT TF\n",
    "# FT FF\n",
    "#\n",
    "# TT = true positive = suggestion provided, clicked through dym\n",
    "# TF = false positive = suggestion provided, user did not click through d\n",
    "# FT = false negative = no suggestion provided, user reformulated query (should have provided suggestion)\n",
    "# FF = true negative = no suggestion provided, clicked through results (no suggestion needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
