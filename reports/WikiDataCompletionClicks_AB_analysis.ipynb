{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wbsearchentities a/b test analysis\n",
    "==\n",
    "\n",
    "As part of [T208917](https://phabricator.wikimedia.org/T208917) the weights of the wbsearchentities prefix search on www.wikidata.org were tuned using historical click logs as a guide. To determine the effectiveness of this tuning an AB test was run from Nov 30 at 00:00 UTC until Dec 7 at 00:00 UTC. The test was limited to users performing searches for wikidata items in the English language. Users were divided equally on a per-page load basis into either the control or test bucket.\n",
    "\n",
    "The analytics for this test collect usage from the various entity selectors used throughout the interface for editing Wikidata. The data also includes usage of the autocomplete on the top-right of all Wikidata pages, but due to a bug in the data collection the usage of the top-right autocomplete was only logged from entity pages.\n",
    "\n",
    "The graphs below are all probability densities and annotated with 95% confidence intervals. The confidence intervals are constructed by running five thousand rounds of the bootstrap method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gzip import GzipFile\n",
    "import pickle\n",
    "import numpy as np\n",
    "try:\n",
    "    from pyspark.sql import functions as F\n",
    "except ImportError:\n",
    "    import findspark\n",
    "    findspark.init('/usr/lib/spark2')\n",
    "    from pyspark.sql import SparkSession, functions as F\n",
    "    spark = SparkSession.builder.master('local').getOrCreate()\n",
    "    \n",
    "import bokeh\n",
    "import bokeh.io\n",
    "import bokeh.palettes\n",
    "import bokeh.plotting\n",
    "import bokeh.transform\n",
    "import IPython.display\n",
    "\n",
    "palette = bokeh.palettes.Spectral[6]\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "def markdown(content):\n",
    "    IPython.display.display(IPython.display.Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = (F.col('month') == 12) & (F.col('day') < 7)\n",
    "nov = (F.col('month') == 11) & (F.col('day') >= 30)\n",
    "date_cond = (F.col('year') == 2018) & (dec | nov)\n",
    "\n",
    "df_raw = (\n",
    "    spark.read.table('event.wikidatacompletionsearchclicks')\n",
    "    .where(\n",
    "        date_cond & \n",
    "        (F.col('event.context') == 'item') &\n",
    "        (F.col('event.language') == 'en'))\n",
    "    # These events are from the very beginning of the test, when not all\n",
    "    # users recieved the new testing code.\n",
    "    .where(F.col('event.bucket').isNotNull())\n",
    "    .where(F.col('event.pageToken').isNotNull())\n",
    "    .select('dt', 'event.*')\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it fair to cap lengths at 10? i dunno ..\n",
    "df_raw['prefixLen'] = df_raw['searchTerm'].str.len().clip(upper=10)\n",
    "df_raw['click'] = df_raw['action'] == 'click'\n",
    "df_raw['start'] = df_raw['action'] == 'session-start'\n",
    "df_raw['dt'] = df_raw['dt'].astype('datetime64')\n",
    "print((df_raw['dt'].min(), df_raw['dt'].max()))\n",
    "df_by_page = (\n",
    "    df_raw\n",
    "    .groupby(['bucket', 'pageToken'])\n",
    "    .agg({\n",
    "        'dt': lambda x: x.iloc[0],\n",
    "        'start': np.any,\n",
    "        'click': np.any,\n",
    "        'clickIndex': np.min,\n",
    "        'prefixLen': np.sum,\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "df_by_page['totalCharsTyped'] = df_by_page['prefixLen']\n",
    "del df_by_page['prefixLen']\n",
    "\n",
    "colors = {bucket: color for bucket, color in zip(df_raw['bucket'].unique(), palette)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event Counts\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_time = (\n",
    "    df_raw[['dt', 'bucket', 'pageToken', 'click', 'start']]\n",
    "    .groupby(['bucket', 'pageToken'])\n",
    "    .agg({\n",
    "        'dt': lambda x: x.iloc[0],\n",
    "        'start': np.any,\n",
    "        'click': np.any,\n",
    "    })\n",
    "    .reset_index()\n",
    "    .set_index('dt')\n",
    "    .groupby('bucket')\n",
    "    .resample(\"D\")\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "for col, title in (('start', 'session start'), ('click', 'click')):\n",
    "    p = bokeh.plotting.figure(\n",
    "        title='Page loads with {} events by day'.format(title).title(),\n",
    "        plot_height=200, x_axis_type='datetime',\n",
    "        toolbar_location=None)\n",
    "    for bucket, g in df_time.reset_index().groupby('bucket'):\n",
    "        p.line('dt', col, source=g, color=colors[bucket], legend=bucket)\n",
    "    bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_events = df_time.groupby('bucket').sum()\n",
    "md = [\n",
    "    'Raw event counts per bucket\\n==\\n',\n",
    "    '|bucket|event|count|',\n",
    "    '|------|-----|-----|',\n",
    "]\n",
    "for idx, row in total_events.iterrows():\n",
    "    for event, count in row.items():\n",
    "        md.append('|{}|{}|{}|'.format(idx, event, int(count)))\n",
    "markdown('\\n'.join(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci(values, rounds=5000, n=None, agg=lambda x: x.mean(axis=1)):\n",
    "    if n is None:\n",
    "        n = len(values)\n",
    "    samples = np.random.choice(values, size=n * rounds, replace=True).reshape(rounds, -1)\n",
    "    scores = np.sort(agg(samples))\n",
    "    alpha = 0.05\n",
    "    low = int(rounds * (alpha/2))\n",
    "    mid = int(rounds / 2)\n",
    "    high = int(rounds * (1 - alpha/2))\n",
    "    return (scores[low], scores[mid], scores[high]), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.kde import gaussian_kde\n",
    "from collections import defaultdict\n",
    "\n",
    "def ridge(bucket, data, scale):\n",
    "    return list(zip([bucket]*len(data), scale*data))\n",
    "\n",
    "def plot_distribution(title, buckets, data):\n",
    "    min_x = min(np.min(raw) for _, raw in data.values())\n",
    "    max_x = max(np.max(raw) for _, raw in data.values())\n",
    "    \n",
    "    x = np.linspace(min_x, max_x, 500)\n",
    "    # A bit evil .. but for the patch to draw the polygon we need\n",
    "    # the data to start and end with y=0. The first and last\n",
    "    # x values are repeated and these are applied manually later.\n",
    "    x = np.append(np.append(x, x[-1])[::-1], x[0])[::-1]\n",
    "    source = bokeh.models.ColumnDataSource(data=dict(x=x))\n",
    "    p = bokeh.plotting.figure(\n",
    "        y_range=sorted(buckets, reverse=True), title=title,\n",
    "        plot_height=75 * len(buckets), plot_width=700,\n",
    "        x_range=(min_x, max_x),\n",
    "        toolbar_location=None)\n",
    "    \n",
    "    pdfs = {bucket: gaussian_kde(raw) for bucket, (_, raw) in data.items()}\n",
    "    ys = {bucket: pdf(x) for bucket, pdf in pdfs.items()}\n",
    "    max_y = max(np.max(ys[bucket]) for bucket in data.keys())\n",
    "    scale = 0.8 / max_y\n",
    "    \n",
    "    bounds_data = defaultdict(list)\n",
    "    for bucket, (bounds, raw) in sorted(data.items(), key=lambda x: x[0], reverse=True):\n",
    "        # Apply polygon minimum edges\n",
    "        ys[bucket][0] = 0\n",
    "        ys[bucket][-1] = 0\n",
    "        y = ridge(bucket, ys[bucket], scale=scale)\n",
    "        source.add(y, bucket)\n",
    "        p.patch(\n",
    "            'x', bucket, color=colors[bucket], line_color=\"black\",\n",
    "            alpha=0.6, source=source)\n",
    "        if bounds:\n",
    "            bounds_data['buckets'].append(bucket)\n",
    "            bounds_data['upper'].append(bounds[-1])\n",
    "            bounds_data['lower'].append(bounds[0])\n",
    "    if bounds_data:\n",
    "        source_error = bokeh.models.ColumnDataSource(bounds_data)\n",
    "        p.add_layout(bokeh.models.Whisker(\n",
    "            dimension=\"width\", line_color=\"black\",\n",
    "            source=source_error, base=\"buckets\", upper=\"upper\", lower=\"lower\"))\n",
    "\n",
    "    p.y_range.range_padding = 0.4\n",
    "    bokeh.io.show(p)\n",
    "    \n",
    "\n",
    "def plot_ci(title, df, extract, rounds=5000):\n",
    "    data = {}\n",
    "    buckets = df['bucket'].unique()\n",
    "    for bucket in sorted(buckets):\n",
    "        samples = extract(df[df['bucket'] == bucket])\n",
    "        data[bucket] = ci(samples, rounds=rounds)\n",
    "    plot_distribution(title, buckets, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of characters typed before success\n",
    "====================================\n",
    "The number of characters typed in each session is a proxy for the amount of effort a user must exert to find the item they are looking for. The 95% CI completely overlaps, suggesting the test treatment had no effect on the number of characters typed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clicks = df_raw[df_raw['click'] == True].copy().dropna()\n",
    "plot_ci('Mean Characters Typed Per Successful Lookup', df_clicks, lambda x: x['prefixLen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clicks_by_page = df_by_page[df_by_page['click'] == True].copy().dropna()\n",
    "plot_ci('Mean Characters Typed Per Page Load', df_clicks_by_page, lambda x: x['totalCharsTyped'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abandonment Rate\n",
    "================\n",
    "The ratio of page loads with start events against the page loads with click events is interepreted loosely as the abandonment rate of search. The 95% CI completely overlaps, suggesting the test treatment had no effect on abandonment rates. This is the first time we've looked at abandonment rates for wbsearchentities, and further investigation into why it is so high may be called for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abandon = (\n",
    "    df_raw\n",
    "    .groupby(['bucket', 'pageToken'])\n",
    "    .agg({'click': np.any})\n",
    "    .reset_index()\n",
    ")\n",
    "df_abandon['abandon'] = 1 - df_abandon['click']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ci('Abandonment Rate', df_abandon, lambda x: x['abandon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click Position\n",
    "===========\n",
    "\n",
    "The position of clicked result is another proxy for the amount of effort a user must exert to find the item they are looking for. The mean position clicked decreased from 1.38 to 1.33, which is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ci('Mean click position', df_clicks, lambda x: 1 + x['clickIndex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into this result closer, the change that occured was an increase in Clicks@1 from 80% to 84%. Clicks@2 saw a comparable drop from 14% to 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    title = 'Percentage of users clicking result position {}'.format(i + 1)\n",
    "    plot_ci(title, df_clicks, lambda x: x['clickIndex'] == i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
