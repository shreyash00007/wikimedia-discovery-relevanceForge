{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikidata entity completion - item / fr\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from gzip import GzipFile\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "from IPython.display import Markdown as md\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import relforge.cli.tf_autocomplete as ac\n",
    "import relforge.tf_optimizer as opt\n",
    "\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = 'item_fr'\n",
    "with GzipFile('/home/ebernhardson/tf-ltr-data/model/model.{}.pkl.gz'.format(variant), 'rb') as f:\n",
    "    min_report = pickle.load(f)\n",
    "with GzipFile('/home/ebernhardson/tf-ltr-data/sensitivity/sensitivity.{}.pkl.gz'.format(variant), 'rb') as f:\n",
    "    sens_report = pickle.load(f)\n",
    "for report in min_report.evaluation_reports + [min_report.initial_report]:\n",
    "    report.scores = {\n",
    "        k: v if isinstance(v, opt.EvaluationScores) else opt.EvaluationScores(v) \n",
    "        for k, v in report.scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.kde import gaussian_kde\n",
    "from collections import defaultdict\n",
    "\n",
    "def ridge(bucket, data, scale):\n",
    "    return list(zip([bucket]*len(data), scale*data))\n",
    "\n",
    "def plot_distribution(title, buckets, data, colors):\n",
    "    min_x = min(np.min(raw) for _, raw in data.values())\n",
    "    max_x = max(np.max(raw) for _, raw in data.values())\n",
    "    \n",
    "    x = np.linspace(min_x, max_x, 500)\n",
    "    # A bit evil .. but for the patch to draw the polygon we need\n",
    "    # the data to start and end with y=0. The first and last\n",
    "    # x values are repeated and these are applied manually later.\n",
    "    x = np.append(np.append(x, x[-1])[::-1], x[0])[::-1]\n",
    "    source = bokeh.models.ColumnDataSource(data=dict(x=x))\n",
    "    p = bokeh.plotting.figure(\n",
    "        y_range=sorted(buckets, reverse=True), title=title,\n",
    "        plot_height=75 * len(buckets), plot_width=700,\n",
    "        x_range=(min_x, max_x),\n",
    "        toolbar_location=None)\n",
    "    \n",
    "    pdfs = {bucket: gaussian_kde(raw) for bucket, (_, raw) in data.items()}\n",
    "    ys = {bucket: pdf(x) for bucket, pdf in pdfs.items()}\n",
    "    max_y = max(np.max(ys[bucket]) for bucket in data.keys())\n",
    "    scale = 0.8 / max_y\n",
    "    \n",
    "    bounds_data = defaultdict(list)\n",
    "    for bucket, (bounds, raw) in sorted(data.items(), key=lambda x: x[0], reverse=True):\n",
    "        # Apply polygon minimum edges\n",
    "        ys[bucket][0] = 0\n",
    "        ys[bucket][-1] = 0\n",
    "        y = ridge(bucket, ys[bucket], scale=scale)\n",
    "        source.add(y, bucket)\n",
    "        p.patch(\n",
    "            'x', bucket, color=colors[bucket], line_color=\"black\",\n",
    "            alpha=0.6, source=source)\n",
    "        if bounds:\n",
    "            bounds_data['buckets'].append(bucket)\n",
    "            bounds_data['upper'].append(bounds[-1])\n",
    "            bounds_data['lower'].append(bounds[0])\n",
    "    if bounds_data:\n",
    "        source_error = bokeh.models.ColumnDataSource(bounds_data)\n",
    "        p.add_layout(bokeh.models.Whisker(\n",
    "            dimension=\"width\", line_color=\"black\",\n",
    "            source=source_error, base=\"buckets\", upper=\"upper\", lower=\"lower\"))\n",
    "\n",
    "    p.y_range.range_padding = 0.4\n",
    "    bokeh.io.show(p)\n",
    "\n",
    "def ci(values, rounds=5000, alpha=0.05, n=None, agg=lambda x: x.mean(axis=1)):\n",
    "    if n is None:\n",
    "        n = len(values)\n",
    "    samples = np.random.choice(values, size=n * rounds, replace=True).reshape(rounds, -1)\n",
    "    scores = np.sort(agg(samples))\n",
    "    low = int(rounds * (alpha/2))\n",
    "    mid = int(rounds / 2)\n",
    "    high = int(rounds * (1 - alpha/2))\n",
    "    return (scores[low], scores[mid], scores[high]), scores\n",
    "\n",
    "def plot_ci(title, df, colors, extract, rounds=1000):\n",
    "    data = {}\n",
    "    buckets = df['bucket'].unique()\n",
    "    for bucket in sorted(buckets):\n",
    "        samples = extract(df[df['bucket'] == bucket])\n",
    "        data[bucket] = ci(samples, rounds=rounds)\n",
    "    plot_distribution(title, buckets, data, colors)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric used\n",
    "===========\n",
    "\n",
    "The metric used here to compare different tuning of the autocomplete algorithm represents the probablistic number of characters typed by a typical user before selecting their desired result from the autocomplete drop down. This metric first looks at real user sessions to estimate how likely a user is to continue typing even when their result is presented in the autocomplete, conditioned on the position the result is displayed at. Individual user sessions, represented in the data as (prefix_typed, page_id_clicked), are then simulated with prefixes from length 1 to the full prefix typed. From this simulation we determine the expected number of characters typed for an individual search clickthrough.\n",
    "\n",
    "The dataset used contains 50k clickthroughs from oct 7 - dec 20 in the training set, and another 50k clicks from dec 20 - jan 7 in the test set.\n",
    "\n",
    "The graph below shows bootstrapped probability densities for each bucket. Tick marks are shown at the 95% confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "initial = min_report.initial_report['test'].scores\n",
    "best = min_report.best_report['test'].scores\n",
    "df = pd.DataFrame({\n",
    "    'bucket': (['initial'] * len(initial)) + (['best'] * len(best)),\n",
    "    'value': np.append(initial, best)\n",
    "})\n",
    "colors = {\n",
    "    'initial': 'blue',\n",
    "    'best': 'orange',\n",
    "}\n",
    "confidence = plot_ci('Mean characters typed probability density', df, colors, lambda x: x['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nearby_reports = [r for r in min_report.evaluation_reports if r.scores['test'].mean < confidence['best'][0][-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean characters typed by percentile\n",
    "==============================\n",
    "The following graphs show the before and after effects of tuning. This shows strong improvement, up to a full character, from percentiles 0-65. Tail queries show some decline in performance, but only slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'initial': min_report.initial_report['test'].percentiles,\n",
    "    'best': min_report.best_report['test'].percentiles,\n",
    "}).reset_index()\n",
    "p = bokeh.plotting.figure()\n",
    "p.line('initial', 'index', source=df, legend='initial score')\n",
    "p.line('best', 'index', source=df, legend='best score', line_color='orange')\n",
    "p.xaxis.axis_label = 'expected characters typed'\n",
    "p.yaxis.axis_label = 'session percentile'\n",
    "bokeh.plotting.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per-session delta in expected characters typed\n",
    "======================================\n",
    "\n",
    "This shows on a per-session basis the change in number of characters typed between the baseline scoring and the scoring after parameter tuning. This suggests up to 20% of sessions save between 1 and 6 characters typed. Another 35% save a fractional character, and 20% have no impact. Around 20% of sessions are impacted negatively, future inspection of what makes these sessions different may be useful for investigating new scoring signals.\n",
    "\n",
    "The orange line shows the training run with the best score. The faint blue lines show other training runs that have a mean score less than the upper 95% CI of the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bokeh.plotting.figure(title='expected characters typed delta')\n",
    "p.xaxis.axis_label = 'expected change in characters typed'\n",
    "p.yaxis.axis_label = 'session percentile'\n",
    "percentiles = np.arange(0, 101)\n",
    "best_report = min_report.best_report\n",
    "for report in nearby_reports:\n",
    "    if report == best_report:\n",
    "        continue\n",
    "    score_delta = report.scores['test'].scores - min_report.initial_report['test'].scores\n",
    "    score_delta_percentile = np.percentile(score_delta, percentiles)\n",
    "    p.line('delta', 'percentile', source={'delta': score_delta_percentile, 'percentile': percentiles}, alpha=0.01)\n",
    "\n",
    "score_delta = best_report.scores['test'].scores - min_report.initial_report['test'].scores\n",
    "percentiles = np.arange(0, 101)\n",
    "score_delta_percentile = np.percentile(score_delta, percentiles)\n",
    "p.line('delta', 'percentile', \n",
    "       source={'delta': score_delta_percentile, 'percentile': percentiles},\n",
    "       color='orange', line_width=2, legend='best score')\n",
    "\n",
    "bokeh.plotting.show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Tuned Values\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in sorted(min_report.summary['best_report']['variables'].items(), key=lambda x: x[0]):\n",
    "    if 'statement_keywords' in k:\n",
    "        # not worth tuning, these are simply deboosts that should be strong enough to do the job\n",
    "        continue\n",
    "    print('{score:10.2f} : {name}'.format(score=v, name=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mean = min_report.best_report['test'].mean\n",
    "top_reports = [x for x in min_report.evaluation_reports if x['test'].mean <= confidence['best'][0][-1]]\n",
    "# [x['test'].mean for x in top_reports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict({'score': [x['test'].mean for x in min_report.evaluation_reports]}, **{\n",
    "    var_name: [x.variables[var_name] for x in min_report.evaluation_reports]\n",
    "    for var_name in min_report.best_report.variables.keys()\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity of chosen parameters\n",
    "===========================\n",
    "\n",
    "To get an idea of how much influence individual parameters have on the final score, and to estimate how sensitive those variables are to small changes, the graphs below plot the sensitivity of individual parameters. This is performed by holding all variables except one as a constant, and sweeping a set of values around the chosen point. The graphs then show how the final score changes based on changes to that variable. Dots on the graphs are additionally colored by their score. A graph of a single color suggests the variable in question has a relatively small influence on the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bokeh.models\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "high = df['score'].mean() + df['score'].std()\n",
    "cmap = bokeh.models.LinearColorMapper(palette=Viridis256, low=np.min(df['score']), high=high)\n",
    "plots = []\n",
    "for var_name, reports in sorted(sens_report.variable_reports.items(), key=lambda x: x[0]):\n",
    "    tested_values = [r.variables[var_name] for r in reports]\n",
    "    scores = [r['test'].mean for r in reports]\n",
    "\n",
    "    p = bokeh.plotting.figure(title=var_name, height=250)\n",
    "    p.circle(x='value', y='score', size=8, alpha=0.5,\n",
    "             fill_color={'field': 'score', 'transform': cmap},\n",
    "             source={'value': tested_values, 'score': scores})\n",
    "    plots.append([p])\n",
    "grid = bokeh.layouts.gridplot(plots)\n",
    "bokeh.plotting.show(grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
